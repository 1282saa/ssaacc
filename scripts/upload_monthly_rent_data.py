#!/usr/bin/env python3
"""
Ï≤≠ÎÖÑÏõîÏÑ∏ Îç∞Ïù¥ÌÑ∞Î•º S3Ïóê ÏóÖÎ°úÎìúÌïòÍ≥† PostgreSQL + pgvectorÏóê ÏûÑÎ≤†Îî©ÌïòÎäî Ïä§ÌÅ¨Î¶ΩÌä∏
"""

import os
import json
import boto3
import psycopg2
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ÏÑ§Ï†ï
DATA_DIR = "/Users/yeong-gwang/Documents/Î∞∞ÏõÄ Ïò§Ï†Ñ 1.38.42/Ïô∏Î∂Ä/Í≥µÎ™®Ï†Ñ/ÏÉàÏãπai/Îç∞Ïù¥ÌÑ∞/Ï≤≠ÎÖÑÏõîÏÑ∏"
S3_BUCKET = "finkurn-policy-data"
S3_PREFIX = "policies/monthly-rent/"

# Database configuration
DB_CONFIG = {
    'host': 'localhost',
    'port': '5432',
    'database': 'finkurn',
    'user': 'postgres',
    'password': 'finkurn2024'
}

# AWS Bedrock client for embeddings
bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')
s3 = boto3.client('s3', region_name='us-east-1')


def generate_embedding(text: str) -> List[float]:
    """Generate embedding using Amazon Titan Embeddings V2"""
    response = bedrock.invoke_model(
        modelId='amazon.titan-embed-text-v2:0',
        body=json.dumps({
            'inputText': text,
            'dimensions': 1024,
            'normalize': True
        })
    )

    response_body = json.loads(response['body'].read())
    return response_body['embedding']


def parse_policy_file(file_path: str) -> Dict[str, Any]:
    """Parse policy text file and extract structured information"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    lines = [line.strip() for line in content.split('\n') if line.strip()]

    # Extract policy name (first non-empty line)
    policy_name = lines[0] if lines else os.path.basename(file_path).replace('.txt', '')

    # Parse content
    policy_data = {
        'policy_name': policy_name,
        'category': 'Ï£ºÍ±∞',
        'region': 'ÏÑúÏö∏',  # Most of these are Seoul-specific
        'deadline': 'ÏÉÅÏãú',
        'summary': '',
        'support_content': '',
        'eligibility': {},
        'application_info': {},
        'additional_info': {},
        'tags': ['Ï≤≠ÎÖÑ', 'ÏõîÏÑ∏', 'Ï£ºÍ±∞ÏßÄÏõê', 'ÏÑúÏö∏'],
        'full_text': content
    }

    # Extract specific sections
    current_section = None
    for line in lines:
        if 'ÏßÄÏõêÎåÄÏÉÅ' in line or 'ÎåÄÏÉÅÏûê' in line:
            current_section = 'eligibility'
        elif 'ÏßÄÏõêÎÇ¥Ïö©' in line or 'ÏßÄÏõêÍ∑úÎ™®' in line:
            current_section = 'support'
        elif 'Ïã†Ï≤≠Î∞©Î≤ï' in line or 'Ïã†Ï≤≠' in line:
            current_section = 'application'
        elif line.startswith('‚Äª'):
            if current_section == 'support':
                policy_data['support_content'] += line + '\n'
        elif current_section == 'eligibility':
            if 'Ïó∞Î†π' in line or 'Îßå' in line:
                policy_data['eligibility']['age'] = line
            elif 'ÏÜåÎìù' in line:
                policy_data['eligibility']['income'] = line
            elif 'Ï£ºÍ±∞' in line or 'Í±∞Ï£º' in line:
                policy_data['eligibility']['residence'] = line
        elif current_section == 'support':
            policy_data['support_content'] += line + '\n'
        elif current_section == 'application':
            if 'http' in line or 'www' in line:
                policy_data['application_info']['website'] = line
            elif 'Ï†ÑÌôî' in line or '‚òé' in line or 'ÏΩú' in line:
                policy_data['application_info']['contact'] = line

    # Generate summary
    summary_parts = []
    if policy_data['support_content']:
        summary_parts.append(policy_data['support_content'].split('\n')[0][:100])
    if policy_data['eligibility']:
        age_info = policy_data['eligibility'].get('age', '')
        if age_info:
            summary_parts.append(age_info)

    policy_data['summary'] = ' '.join(summary_parts) if summary_parts else policy_name

    return policy_data


def upload_to_s3(file_path: str, policy_name: str) -> tuple:
    """Upload file to S3 and return bucket/key"""
    file_name = os.path.basename(file_path)
    s3_key = f"{S3_PREFIX}{file_name}"

    try:
        s3.upload_file(file_path, S3_BUCKET, s3_key)
        print(f"‚úÖ Uploaded {file_name} to S3: s3://{S3_BUCKET}/{s3_key}")
        return S3_BUCKET, s3_key
    except Exception as e:
        print(f"‚ùå Failed to upload {file_name} to S3: {e}")
        return None, None


def insert_to_database(policy_data: Dict[str, Any], s3_bucket: str, s3_key: str):
    """Insert policy data with embedding into PostgreSQL"""
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()

    try:
        # Generate embedding
        embedding_text = f"{policy_data['policy_name']} {policy_data['summary']} {policy_data['support_content']}"
        print(f"üîÑ Generating embedding for: {policy_data['policy_name'][:50]}...")
        embedding = generate_embedding(embedding_text)
        embedding_str = '[' + ','.join(map(str, embedding)) + ']'

        # Check if policy already exists
        cursor.execute("SELECT id FROM youth_policies WHERE policy_name = %s", (policy_data['policy_name'],))
        existing = cursor.fetchone()

        if existing:
            print(f"   ‚ö†Ô∏è  Policy already exists, skipping: {policy_data['policy_name']}")
            return

        # Generate filename from policy name
        filename = policy_data['policy_name'].replace(' ', '_')[:100] + '.txt'

        # Insert data
        cursor.execute("""
            INSERT INTO youth_policies (
                policy_name,
                filename,
                full_text,
                category,
                region,
                deadline,
                summary,
                support_content,
                eligibility,
                application_info,
                additional_info,
                tags,
                s3_bucket,
                s3_key,
                embedding
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s::vector
            )
        """, (
            policy_data['policy_name'],
            filename,
            policy_data['full_text'],
            policy_data['category'],
            policy_data['region'],
            policy_data['deadline'],
            policy_data['summary'],
            policy_data['support_content'],
            json.dumps(policy_data['eligibility'], ensure_ascii=False),
            json.dumps(policy_data['application_info'], ensure_ascii=False),
            json.dumps(policy_data['additional_info'], ensure_ascii=False),
            policy_data['tags'],
            s3_bucket,
            s3_key,
            embedding_str
        ))

        conn.commit()
        print(f"‚úÖ Inserted to database: {policy_data['policy_name']}")

    except Exception as e:
        print(f"‚ùå Failed to insert {policy_data['policy_name']}: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
        conn.close()


def main():
    print("=" * 80)
    print("Ï≤≠ÎÖÑÏõîÏÑ∏ Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú & ÏûÑÎ≤†Îî© Ïä§ÌÅ¨Î¶ΩÌä∏")
    print("=" * 80)
    print()

    # Get all .txt files
    data_path = Path(DATA_DIR)
    txt_files = list(data_path.glob("*.txt"))

    print(f"üìÅ Found {len(txt_files)} policy files")
    print()

    for txt_file in txt_files:
        print(f"\n{'=' * 80}")
        print(f"Processing: {txt_file.name}")
        print(f"{'=' * 80}")

        try:
            # Parse policy file
            print("1Ô∏è‚É£  Parsing policy data...")
            policy_data = parse_policy_file(str(txt_file))
            print(f"   Policy: {policy_data['policy_name']}")
            print(f"   Category: {policy_data['category']}")
            print(f"   Summary: {policy_data['summary'][:100]}...")

            # Upload to S3 (optional - skip if bucket doesn't exist)
            print("\n2Ô∏è‚É£  Uploading to S3...")
            try:
                s3_bucket, s3_key = upload_to_s3(str(txt_file), policy_data['policy_name'])
            except Exception as e:
                print(f"   ‚ö†Ô∏è  S3 upload skipped: {e}")
                s3_bucket, s3_key = None, str(txt_file)

            # Insert to database with embedding
            print("\n3Ô∏è‚É£  Inserting to PostgreSQL + pgvector...")
            insert_to_database(policy_data, s3_bucket, s3_key)

            print(f"\n‚úÖ Successfully processed: {txt_file.name}")

        except Exception as e:
            print(f"\n‚ùå Error processing {txt_file.name}: {e}")
            import traceback
            traceback.print_exc()
            continue

    print("\n" + "=" * 80)
    print("‚úÖ All files processed!")
    print("=" * 80)

    # Verify insertions
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    cursor.execute("""
        SELECT COUNT(*)
        FROM youth_policies
        WHERE category = 'Ï£ºÍ±∞' AND embedding IS NOT NULL
    """)
    count = cursor.fetchone()[0]
    cursor.close()
    conn.close()

    print(f"\nüìä Total housing policies with embeddings: {count}")
    print()


if __name__ == '__main__':
    main()
