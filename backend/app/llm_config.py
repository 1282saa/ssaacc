"""
LLM ì„¤ì • ë° ì´ˆê¸°í™”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ì—­í• :
- AWS Bedrock ë˜ëŠ” ì§ì ‘ APIë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ì´ˆê¸°í™”
- í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì ì ˆí•œ LLM ì„ íƒ
- ëª¨ë“  ì—ì´ì „íŠ¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©

## ì§€ì›í•˜ëŠ” LLM:
1. **AWS Bedrock (ì¶”ì²œ)**:
   - Claude 3.5 Sonnet (ì¶”ë¡ ìš©)
   - Titan Embeddings (ì„ë² ë”©ìš©)
   - AWS ìê²© ì¦ëª…ë§Œ í•„ìš” (ë³„ë„ API í‚¤ ë¶ˆí•„ìš”)

2. **Direct API**:
   - Anthropic Claude API (ì¶”ë¡ ìš©)
   - OpenAI API (ì„ë² ë”©ìš©)
   - ê°ê° API í‚¤ í•„ìš”

## ì‚¬ìš© ë°©ë²•:
```python
from app.llm_config import get_chat_llm, get_embeddings

# ì±„íŒ…ìš© LLM ê°€ì ¸ì˜¤ê¸°
llm = get_chat_llm()

# ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
embeddings = get_embeddings()
```

## í™˜ê²½ ë³€ìˆ˜:
- USE_AWS_BEDROCK=true â†’ AWS Bedrock ì‚¬ìš©
- USE_AWS_BEDROCK=false â†’ ì§ì ‘ API ì‚¬ìš©
"""

import os
from typing import Optional
from loguru import logger


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

USE_AWS_BEDROCK = os.getenv("USE_AWS_BEDROCK", "false").lower() == "true"


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# LLM ì´ˆê¸°í™” í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def get_chat_llm(temperature: float = 0.7, max_tokens: int = 4000):
    """
    ì±„íŒ…ìš© LLM ê°€ì ¸ì˜¤ê¸°

    ## ì„¤ëª…:
    í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ AWS Bedrock ë˜ëŠ” ì§ì ‘ Anthropic APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    ## Args:
        temperature (float): ì‘ë‹µì˜ ë¬´ì‘ìœ„ì„± (0.0~1.0)
            - 0.0: ë§¤ìš° ê²°ì •ì 
            - 1.0: ë§¤ìš° ì°½ì˜ì 
        max_tokens (int): ìµœëŒ€ í† í° ìˆ˜

    ## Returns:
        ChatModel: LangChain í˜¸í™˜ ì±„íŒ… ëª¨ë¸

    ## ì‚¬ìš© ì˜ˆì‹œ:
    ```python
    llm = get_chat_llm(temperature=0.3)  # ì¼ê´€ëœ ì‘ë‹µ
    response = await llm.ainvoke([
        SystemMessage(content="You are a helpful assistant"),
        HumanMessage(content="Hello!")
    ])
    ```
    """
    if USE_AWS_BEDROCK:
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì˜µì…˜ 1: AWS Bedrock ì‚¬ìš©
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        try:
            from langchain_aws import ChatBedrock
            import boto3

            logger.info("ğŸ”§ AWS Bedrockìœ¼ë¡œ LLM ì´ˆê¸°í™”")

            # AWS ìê²© ì¦ëª… (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ~/.aws/credentials)
            session = boto3.Session(
                aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
                region_name=os.getenv("AWS_REGION", "us-east-1"),
            )

            bedrock_runtime = session.client("bedrock-runtime")

            # Claude 3.5 Sonnet on AWS Bedrock
            llm = ChatBedrock(
                client=bedrock_runtime,
                model_id="anthropic.claude-3-5-sonnet-20241022-v2:0",  # Bedrock ëª¨ë¸ ID
                model_kwargs={
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                },
            )

            logger.info("âœ… AWS Bedrock Claude 3.5 Sonnet ì´ˆê¸°í™” ì™„ë£Œ")
            return llm

        except Exception as e:
            logger.error(f"âŒ AWS Bedrock ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.warning("âš ï¸  Direct APIë¡œ í´ë°±í•©ë‹ˆë‹¤")
            # í´ë°±: ì§ì ‘ API ì‚¬ìš©
            pass

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì˜µì…˜ 2: Direct Anthropic API ì‚¬ìš©
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    from langchain_anthropic import ChatAnthropic

    logger.info("ğŸ”§ Direct Anthropic APIë¡œ LLM ì´ˆê¸°í™”")

    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
        temperature=temperature,
        max_tokens=max_tokens,
    )

    logger.info("âœ… Anthropic Claude 3.5 Sonnet ì´ˆê¸°í™” ì™„ë£Œ")
    return llm


def get_embeddings():
    """
    ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°

    ## ì„¤ëª…:
    í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ AWS Bedrock Titan ë˜ëŠ” OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    ## Returns:
        Embeddings: LangChain í˜¸í™˜ ì„ë² ë”© ëª¨ë¸

    ## ì‚¬ìš© ì˜ˆì‹œ:
    ```python
    embeddings = get_embeddings()
    vectors = embeddings.embed_documents([
        "í…ìŠ¤íŠ¸ 1",
        "í…ìŠ¤íŠ¸ 2"
    ])
    ```
    """
    if USE_AWS_BEDROCK:
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì˜µì…˜ 1: AWS Bedrock Titan Embeddings
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        try:
            from langchain_aws import BedrockEmbeddings
            import boto3

            logger.info("ğŸ”§ AWS Bedrockìœ¼ë¡œ Embeddings ì´ˆê¸°í™”")

            # AWS ìê²© ì¦ëª…
            session = boto3.Session(
                aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
                region_name=os.getenv("AWS_REGION", "us-east-1"),
            )

            bedrock_runtime = session.client("bedrock-runtime")

            # Titan Embeddings V2
            embeddings = BedrockEmbeddings(
                client=bedrock_runtime,
                model_id="amazon.titan-embed-text-v2:0",  # 1024 ì°¨ì›
            )

            logger.info("âœ… AWS Bedrock Titan Embeddings ì´ˆê¸°í™” ì™„ë£Œ")
            return embeddings

        except Exception as e:
            logger.error(f"âŒ AWS Bedrock Embeddings ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.warning("âš ï¸  OpenAIë¡œ í´ë°±í•©ë‹ˆë‹¤")
            pass

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì˜µì…˜ 2: OpenAI Embeddings
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    from langchain_openai import OpenAIEmbeddings

    logger.info("ğŸ”§ OpenAI APIë¡œ Embeddings ì´ˆê¸°í™”")

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",  # 3072 ì°¨ì›
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )

    logger.info("âœ… OpenAI Embeddings ì´ˆê¸°í™” ì™„ë£Œ")
    return embeddings


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í—¬í¼ í•¨ìˆ˜
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


def get_embedding_dimensions() -> int:
    """
    í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì› ë°˜í™˜

    ## Returns:
        int: ì„ë² ë”© ì°¨ì› ìˆ˜
            - AWS Bedrock Titan: 1024
            - OpenAI text-embedding-3-large: 3072
    """
    if USE_AWS_BEDROCK:
        return 1024  # Titan Embeddings V2
    else:
        return 3072  # OpenAI text-embedding-3-large


def print_llm_config():
    """
    í˜„ì¬ LLM ì„¤ì • ì¶œë ¥ (ë””ë²„ê¹…ìš©)

    ## ì¶œë ¥ ì˜ˆì‹œ:
    ```
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    LLM Configuration
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    USE_AWS_BEDROCK: true
    Chat LLM: AWS Bedrock Claude 3.5 Sonnet
    Embeddings: AWS Bedrock Titan V2 (1024d)
    AWS Region: us-east-1
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ```
    """
    print("â”" * 60)
    print("LLM Configuration")
    print("â”" * 60)
    print(f"USE_AWS_BEDROCK: {USE_AWS_BEDROCK}")

    if USE_AWS_BEDROCK:
        print(f"Chat LLM: AWS Bedrock Claude 3.5 Sonnet")
        print(f"Embeddings: AWS Bedrock Titan V2 (1024d)")
        print(f"AWS Region: {os.getenv('AWS_REGION', 'us-east-1')}")
    else:
        print(f"Chat LLM: Direct Anthropic API")
        print(f"Embeddings: OpenAI text-embedding-3-large (3072d)")

    print("â”" * 60)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    import asyncio
    from langchain_core.messages import HumanMessage

    async def test_llm():
        """LLM ì„¤ì • í…ŒìŠ¤íŠ¸"""
        print_llm_config()

        # Chat LLM í…ŒìŠ¤íŠ¸
        print("\n[Test 1] Chat LLM")
        llm = get_chat_llm(temperature=0.7)
        response = await llm.ainvoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")])
        print(f"ì‘ë‹µ: {response.content[:100]}...")

        # Embeddings í…ŒìŠ¤íŠ¸
        print("\n[Test 2] Embeddings")
        embeddings = get_embeddings()
        vector = embeddings.embed_query("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
        print(f"ì„ë² ë”© ì°¨ì›: {len(vector)}")

    # asyncio.run(test_llm())
